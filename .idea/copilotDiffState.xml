<?xml version="1.0" encoding="UTF-8"?>
<project version="4">
  <component name="CopilotDiffPersistence">
    <option name="pendingDiffs">
      <map>
        <entry key="$PROJECT_DIR$/01_cnn/01_scratch/modules/conv_classifier.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/01_cnn/01_scratch/modules/conv_classifier.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Otto-Friedrich University of Bamberg&#10;Chair of Explainable Machine Learning (xAI)&#10;Deep Learning Assignments&#10;&#10;@description:&#10;Script ties all modules together to create the convolutional neural network.&#10;&#10;@author: Sebastian Doerrich&#10;@copyright: Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg&#10;@credits: [Christian Ledig, Sebastian Doerrich]&#10;@license: CC BY-SA&#10;@version: 1.0&#10;@python: Python 3&#10;@maintainer: Sebastian Doerrich&#10;@email: sebastian.doerrich@uni-bamberg.de&#10;@status: Production&#10;&quot;&quot;&quot;&#10;&#10;from .softmax_ce import SoftmaxCrossEntropy&#10;from .relu import ReLU&#10;from .max_pool import MaxPooling&#10;from .convolution import Conv2D&#10;from .linear import Linear&#10;&#10;&#10;class ConvNet:&#10;    '''&#10;    Max Pooling of input&#10;    '''&#10;    def __init__(self, modules, criterion):&#10;        self.modules = []&#10;        for m in modules:&#10;            if m['type'] == 'Conv2D':&#10;                self.modules.append(&#10;                    Conv2D(m['in_channels'],&#10;                           m['out_channels'],&#10;                           m['kernel_size'],&#10;                           m['stride'],&#10;                           m['padding'])&#10;                )&#10;            elif m['type'] == 'ReLU':&#10;                self.modules.append(&#10;                    ReLU()&#10;                )&#10;            elif m['type'] == 'MaxPooling':&#10;                self.modules.append(&#10;                    MaxPooling(m['kernel_size'],&#10;                               m['stride'])&#10;                )&#10;            elif m['type'] == 'Linear':&#10;                self.modules.append(&#10;                    Linear(m['in_dim'],&#10;                           m['out_dim'])&#10;                )&#10;        if criterion['type'] == 'SoftmaxCrossEntropy':&#10;            self.criterion = SoftmaxCrossEntropy()&#10;        else:&#10;            raise ValueError(&quot;Wrong Criterion Passed&quot;)&#10;&#10;    def forward(self, x, y):&#10;        &quot;&quot;&quot;&#10;        The forward pass of the model.&#10;&#10;        :param x: input data: (N, C, H, W)&#10;        :param y: input label: (N, )&#10;&#10;        :return:&#10;            probs: the probabilities of all classes: (N, num_classes)&#10;            loss: the cross entropy loss&#10;        &quot;&quot;&quot;&#10;&#10;        probs = None&#10;        loss = None&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement forward pass of the model                                 #&#10;        #############################################################################&#10;&#10;        # Forward pass through all modules&#10;        for module in self.modules:&#10;            x = module.forward(x)&#10;&#10;        probs = x  # Set final output after the last layer&#10;        loss = self.criterion.forward(probs, y)&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;        return probs, loss&#10;&#10;    def backward(self):&#10;        &quot;&quot;&quot;&#10;        The backward pass of the model.&#10;&#10;        :return: nothing but dx, dw, and db of all modules are updated&#10;        &quot;&quot;&quot;&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement backward pass of the model                                #&#10;        #############################################################################&#10;&#10;        dx = self.criterion.backward()  # Compute initial gradient&#10;        for module in reversed(self.modules):&#10;            dx = module.backward(dx)  # Back-propagate through all layers in reverse&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Otto-Friedrich University of Bamberg&#10;Chair of Explainable Machine Learning (xAI)&#10;Deep Learning Assignments&#10;&#10;@description:&#10;Script ties all modules together to create the convolutional neural network.&#10;&#10;@author: Sebastian Doerrich&#10;@copyright: Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg&#10;@credits: [Christian Ledig, Sebastian Doerrich]&#10;@license: CC BY-SA&#10;@version: 1.0&#10;@python: Python 3&#10;@maintainer: Sebastian Doerrich&#10;@email: sebastian.doerrich@uni-bamberg.de&#10;@status: Production&#10;&quot;&quot;&quot;&#10;&#10;from .softmax_ce import SoftmaxCrossEntropy&#10;from .relu import ReLU&#10;from .max_pool import MaxPooling&#10;from .convolution import Conv2D&#10;from .linear import Linear&#10;&#10;&#10;class ConvNet:&#10;    '''&#10;    Max Pooling of input&#10;    '''&#10;    def __init__(self, modules, criterion):&#10;        self.modules = []&#10;        for m in modules:&#10;            if m['type'] == 'Conv2D':&#10;                self.modules.append(&#10;                    Conv2D(m['in_channels'],&#10;                           m['out_channels'],&#10;                           m['kernel_size'],&#10;                           m['stride'],&#10;                           m['padding'])&#10;                )&#10;            elif m['type'] == 'ReLU':&#10;                self.modules.append(&#10;                    ReLU()&#10;                )&#10;            elif m['type'] == 'MaxPooling':&#10;                self.modules.append(&#10;                    MaxPooling(m['kernel_size'],&#10;                               m['stride'])&#10;                )&#10;            elif m['type'] == 'Linear':&#10;                self.modules.append(&#10;                    Linear(m['in_dim'],&#10;                           m['out_dim'])&#10;                )&#10;        if criterion['type'] == 'SoftmaxCrossEntropy':&#10;            self.criterion = SoftmaxCrossEntropy()&#10;        else:&#10;            raise ValueError(&quot;Wrong Criterion Passed&quot;)&#10;&#10;    def forward(self, x, y):&#10;        &quot;&quot;&quot;&#10;        The forward pass of the model.&#10;&#10;        :param x: input data: (N, C, H, W)&#10;        :param y: input label: (N, )&#10;&#10;        :return:&#10;            probs: the probabilities of all classes: (N, num_classes)&#10;            loss: the cross entropy loss&#10;        &quot;&quot;&quot;&#10;&#10;        probs = None&#10;        loss = None&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement forward pass of the model                                 #&#10;        #############################################################################&#10;&#10;        # Forward pass through all modules&#10;        for module in self.modules:&#10;            x = module.forward(x)&#10;&#10;        probs = x  # Set final output after the last layer&#10;        loss = self.criterion.forward(probs, y)&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;        return probs, loss&#10;&#10;    def backward(self):&#10;        &quot;&quot;&quot;&#10;        The backward pass of the model.&#10;&#10;        :return: nothing but dx, dw, and db of all modules are updated&#10;        &quot;&quot;&quot;&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement backward pass of the model                                #&#10;        #############################################################################&#10;&#10;        dx = self.criterion.backward()  # Compute initial gradient&#10;        for module in reversed(self.modules):&#10;            dx = module.backward(dx)  # Back-propagate through all layers in reverse&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/01_cnn/01_scratch/modules/convolution.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/01_cnn/01_scratch/modules/convolution.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Otto-Friedrich University of Bamberg&#10;Chair of Explainable Machine Learning (xAI)&#10;Deep Learning Assignments&#10;&#10;@description:&#10;Script ties all modules together to create the convolutional neural network.&#10;&#10;@author: Sebastian Doerrich&#10;@copyright: Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg&#10;@credits: [Christian Ledig, Sebastian Doerrich]&#10;@license: CC BY-SA&#10;@version: 1.0&#10;@python: Python 3&#10;@maintainer: Sebastian Doerrich&#10;@email: sebastian.doerrich@uni-bamberg.de&#10;@status: Production&#10;&quot;&quot;&quot;&#10;&#10;import numpy as np&#10;&#10;&#10;class Conv2D:&#10;    &quot;&quot;&quot;&#10;    An implementation of the convolutional layer. We convolve the input with out_channels different filters&#10;    and each filter spans all channels in the input.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):&#10;        &quot;&quot;&quot;&#10;        :param in_channels: the number of channels of the input data&#10;        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)&#10;        :param kernel_size: the specified size of the kernel(both height and width)&#10;        :param stride: the stride of convolution&#10;        :param padding: the size of padding. Pad zeros to the input with padding size.&#10;        &quot;&quot;&quot;&#10;&#10;        self.in_channels = in_channels&#10;        self.out_channels = out_channels&#10;        self.kernel_size = kernel_size&#10;        self.stride = stride&#10;        self.padding = padding&#10;&#10;        self.cache = None&#10;&#10;        self._init_weights()&#10;&#10;    def _init_weights(self):&#10;        np.random.seed(1024)&#10;        self.weight = 1e-3 * np.random.randn(self.out_channels, self.in_channels,  self.kernel_size, self.kernel_size)&#10;        self.bias = np.zeros(self.out_channels)&#10;&#10;        self.dx = None&#10;        self.dw = None&#10;        self.db = None&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;&#10;        The forward pass of convolution.&#10;&#10;        :param x: input data of shape (N, C, H, W)&#10;        :return: output data of shape (N, self.out_channels, H', W') where H' and W' are determined by the convolution&#10;                 parameters. Save necessary variables in self.cache for backward pass&#10;        &quot;&quot;&quot;&#10;&#10;        out = None&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the convolution forward pass.                             #&#10;        #                                                                           #&#10;        # Hint: 1) You may use np.pad for padding.                                  #&#10;        #       2) You may implement the convolution with loops                     #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        F = self.out_channels&#10;        HH = WW = self.kernel_size&#10;        pad = self.padding&#10;        stride = self.stride&#10;&#10;        # Calculate output dimensions&#10;        H_Out = 1 + (H + 2 * pad - HH) // stride&#10;        W_Out = 1 + (W + 2 * pad - WW) // stride&#10;&#10;        # Initialize ouput&#10;        out = np.zeros((N, F, H_Out, W_Out))&#10;&#10;        x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)&#10;&#10;        for n in range(N):&#10;            for f in range(F):&#10;                for i in range(H_Out):&#10;                    for j in range(W_Out):&#10;                        h_start = i * stride&#10;                        h_end = h_start + HH&#10;                        w_start = j * stride&#10;                        w_end = w_start + WW&#10;&#10;                        # Get input slice&#10;                        input_slice = x_padded[n, :, h_start:h_end, w_start:w_end]&#10;&#10;                        # Compute convolution for this position&#10;                        out[n, f, i, j] = np.sum(input_slice * self.weight[f]) + self.bias[f]&#10;&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;        self.cache = x&#10;        return out&#10;&#10;    def backward(self, dout):&#10;        &quot;&quot;&quot;&#10;        The backward pass of convolution&#10;&#10;        :param dout: upstream gradients&#10;&#10;        :return: nothing but dx, dw, and db of self should be updated&#10;        &quot;&quot;&quot;&#10;&#10;        x = self.cache&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the convolution backward pass.                            #&#10;        #                                                                           #&#10;        # Hint:                                                                     #&#10;        #       1) You may implement the convolution with loops                     #&#10;        #       2) don't forget padding when computing dx                           #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        F, _, HH, WW = self.weight.shape&#10;        H_Out, W_Out = dout.shape[2], dout.shape[3]&#10;        pad = self.padding&#10;        stride = self.stride&#10;&#10;        # Initialize gradients&#10;        self.dx = np.zeros_like(x)&#10;        self.dw = np.zeros_like(self.weight)&#10;        self.db = np.zeros_like(self.bias)&#10;&#10;        x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)&#10;        dx_padded = np.zeros_like(x_padded)&#10;&#10;        for n in range(N):&#10;            for f in range(F):&#10;                for i in range(H_Out):&#10;                    for j in range(W_Out):&#10;                        h_start = i * stride&#10;                        h_end = h_start + HH&#10;                        w_start = j * stride&#10;                        w_end = w_start + WW&#10;&#10;                        # Get input slice&#10;                        x_slice = x_padded[n, :, h_start:h_end, w_start:w_end]&#10;&#10;                        # Calculate gradient of weight&#10;                        self.dw[f] += x_slice * dout[n, f, i, j]&#10;&#10;                        # Calculate gradient of bias&#10;                        self.db[f] += dout[n, f, i, j]&#10;&#10;                        # Calculate gradient of input&#10;                        dx_padded[n, :, h_start:h_end, w_start:w_end] += self.weight[f] * dout[n, f, i, j]&#10;&#10;        # Remove padding from dx_padded to get dx&#10;        if pad == 0:&#10;            self.dx = dx_padded&#10;        else:&#10;            self.dx = dx_padded[:, :, pad:-pad, pad:-pad]&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Otto-Friedrich University of Bamberg&#10;Chair of Explainable Machine Learning (xAI)&#10;Deep Learning Assignments&#10;&#10;@description:&#10;Script ties all modules together to create the convolutional neural network.&#10;&#10;@author: Sebastian Doerrich&#10;@copyright: Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg&#10;@credits: [Christian Ledig, Sebastian Doerrich]&#10;@license: CC BY-SA&#10;@version: 1.0&#10;@python: Python 3&#10;@maintainer: Sebastian Doerrich&#10;@email: sebastian.doerrich@uni-bamberg.de&#10;@status: Production&#10;&quot;&quot;&quot;&#10;&#10;import numpy as np&#10;&#10;&#10;class Conv2D:&#10;    &quot;&quot;&quot;&#10;    An implementation of the convolutional layer. We convolve the input with out_channels different filters&#10;    and each filter spans all channels in the input.&#10;    &quot;&quot;&quot;&#10;    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=0):&#10;        &quot;&quot;&quot;&#10;        :param in_channels: the number of channels of the input data&#10;        :param out_channels: the number of channels of the output(aka the number of filters applied in the layer)&#10;        :param kernel_size: the specified size of the kernel(both height and width)&#10;        :param stride: the stride of convolution&#10;        :param padding: the size of padding. Pad zeros to the input with padding size.&#10;        &quot;&quot;&quot;&#10;&#10;        self.in_channels = in_channels&#10;        self.out_channels = out_channels&#10;        self.kernel_size = kernel_size&#10;        self.stride = stride&#10;        self.padding = padding&#10;&#10;        self.cache = None&#10;&#10;        self._init_weights()&#10;&#10;    def _init_weights(self):&#10;        np.random.seed(1024)&#10;        self.weight = 1e-3 * np.random.randn(self.out_channels, self.in_channels,  self.kernel_size, self.kernel_size)&#10;        self.bias = np.zeros(self.out_channels)&#10;&#10;        self.dx = None&#10;        self.dw = None&#10;        self.db = None&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;&#10;        The forward pass of convolution.&#10;&#10;        :param x: input data of shape (N, C, H, W)&#10;        :return: output data of shape (N, self.out_channels, H', W') where H' and W' are determined by the convolution&#10;                 parameters. Save necessary variables in self.cache for backward pass&#10;        &quot;&quot;&quot;&#10;&#10;        out = None&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the convolution forward pass.                             #&#10;        #                                                                           #&#10;        # Hint: 1) You may use np.pad for padding.                                  #&#10;        #       2) You may implement the convolution with loops                     #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        F = self.out_channels&#10;        HH = WW = self.kernel_size&#10;        pad = self.padding&#10;        stride = self.stride&#10;&#10;        # Calculate output dimensions&#10;        H_Out = 1 + (H + 2 * pad - HH) // stride&#10;        W_Out = 1 + (W + 2 * pad - WW) // stride&#10;&#10;        # Initialize ouput&#10;        out = np.zeros((N, F, H_Out, W_Out))&#10;&#10;        x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)&#10;&#10;        for n in range(N):&#10;            for f in range(F):&#10;                for i in range(H_Out):&#10;                    for j in range(W_Out):&#10;                        h_start = i * stride&#10;                        h_end = h_start + HH&#10;                        w_start = j * stride&#10;                        w_end = w_start + WW&#10;&#10;                        # Get input slice&#10;                        input_slice = x_padded[n, :, h_start:h_end, w_start:w_end]&#10;&#10;                        # Compute convolution for this position&#10;                        out[n, f, i, j] = np.sum(input_slice * self.weight[f]) + self.bias[f]&#10;&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;        self.cache = x&#10;        return out&#10;&#10;    def backward(self, dout):&#10;        &quot;&quot;&quot;&#10;        The backward pass of convolution&#10;&#10;        :param dout: upstream gradients&#10;&#10;        :return: nothing but dx, dw, and db of self should be updated&#10;        &quot;&quot;&quot;&#10;&#10;        x = self.cache&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the convolution backward pass.                            #&#10;        #                                                                           #&#10;        # Hint:                                                                     #&#10;        #       1) You may implement the convolution with loops                     #&#10;        #       2) don't forget padding when computing dx                           #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        F, _, HH, WW = self.weight.shape&#10;        H_Out, W_Out = dout.shape[2], dout.shape[3]&#10;        pad = self.padding&#10;        stride = self.stride&#10;&#10;        # Initialize gradients&#10;        self.dx = np.zeros_like(x)&#10;        self.dw = np.zeros_like(self.weight)&#10;        self.db = np.zeros_like(self.bias)&#10;&#10;        x_padded = np.pad(x, ((0, 0), (0, 0), (pad, pad), (pad, pad)), mode='constant', constant_values=0)&#10;        dx_padded = np.zeros_like(x_padded)&#10;&#10;        for n in range(N):&#10;            for f in range(F):&#10;                for i in range(H_Out):&#10;                    for j in range(W_Out):&#10;                        h_start = i * stride&#10;                        h_end = h_start + HH&#10;                        w_start = j * stride&#10;                        w_end = w_start + WW&#10;&#10;                        # Get input slice&#10;                        x_slice = x_padded[n, :, h_start:h_end, w_start:w_end]&#10;&#10;                        # Calculate gradient of weight&#10;                        self.dw[f] += x_slice * dout[n, f, i, j]&#10;&#10;                        # Calculate gradient of bias&#10;                        self.db[f] += dout[n, f, i, j]&#10;&#10;                        # Calculate gradient of input&#10;                        dx_padded[n, :, h_start:h_end, w_start:w_end] += self.weight[f] * dout[n, f, i, j]&#10;&#10;        # Remove padding from dx_padded to get dx&#10;        if pad == 0:&#10;            self.dx = dx_padded&#10;        else:&#10;            self.dx = dx_padded[:, :, pad:-pad, pad:-pad]&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################" />
            </PendingDiffInfo>
          </value>
        </entry>
        <entry key="$PROJECT_DIR$/01_cnn/01_scratch/modules/max_pool.py">
          <value>
            <PendingDiffInfo>
              <option name="filePath" value="$PROJECT_DIR$/01_cnn/01_scratch/modules/max_pool.py" />
              <option name="originalContent" value="&quot;&quot;&quot;&#10;Otto-Friedrich University of Bamberg&#10;Chair of Explainable Machine Learning (xAI)&#10;Deep Learning Assignments&#10;&#10;@description:&#10;Implementation of the max pooling operation.&#10;&#10;@author: Sebastian Doerrich&#10;@copyright: Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg&#10;@credits: [Christian Ledig, Sebastian Doerrich]&#10;@license: CC BY-SA&#10;@version: 1.0&#10;@python: Python 3&#10;@maintainer: Sebastian Doerrich&#10;@email: sebastian.doerrich@uni-bamberg.de&#10;@status: Production&#10;&quot;&quot;&quot;&#10;&#10;import numpy as np&#10;&#10;&#10;class MaxPooling:&#10;    &quot;&quot;&quot;&#10;    Max Pooling of input&#10;    &quot;&quot;&quot;&#10;    def __init__(self, kernel_size, stride):&#10;        self.kernel_size = kernel_size&#10;        self.stride = stride&#10;        self.cache = None&#10;        self.dx = None&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;&#10;        Forward pass of max pooling.&#10;&#10;        :param x: input, (N, C, H, W)&#10;&#10;        :return: The output by max pooling with kernel_size and stride&#10;        &quot;&quot;&quot;&#10;&#10;        out = None&#10;        H_out, W_out = None, None&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the max pooling forward pass.                             #&#10;        #                                                                           #&#10;        # Hint:                                                                     #&#10;        #    1) You may implement the process with loops                            #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        HH, WW = self.kernel_size, self.kernel_size&#10;        stride = self.stride&#10;        H_out = (H - HH) // stride + 1&#10;        W_out = (W - WW) // stride + 1&#10;&#10;        out = np.zeros((N, C, H_out, W_out))&#10;&#10;        for n in range(N):&#10;            for c in range(C):&#10;                for i in range(H_out):&#10;                    for j in range(W_out):&#10;                        h_start, w_start = i * stride, j * stride&#10;                        h_end, w_end = h_start + HH, w_start + WW&#10;                        out[n, c, i, j] = np.max(x[n, c, h_start:h_end, w_start:w_end])&#10;&#10;        H_out, W_out = out.shape[2], out.shape[3]&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;        self.cache = (x, H_out, W_out)&#10;        return out&#10;&#10;    def backward(self, dout):&#10;        &quot;&quot;&quot;&#10;        Backward pass of max pooling.&#10;&#10;        :param dout: Upstream derivatives&#10;        &quot;&quot;&quot;&#10;&#10;        x, H_out, W_out = self.cache&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the max pooling backward pass.                            #&#10;        #                                                                           #&#10;        # Hint:                                                                     #&#10;        #    1) You may implement the process with loops                            #&#10;        #    2) You may find np.unravel_index useful                                #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        HH, WW = self.kernel_size, self.kernel_size&#10;        stride = self.stride&#10;        H_out, W_out = dout.shape[2], dout.shape[3]&#10;&#10;        self.dx = np.zeros_like(x)&#10;&#10;        for n in range(N):&#10;            for c in range(C):&#10;                for i in range(H_out):&#10;                    for j in range(W_out):&#10;                        h_start, w_start = i * stride, j * stride&#10;                        h_end, w_end = h_start + HH, w_start + WW&#10;                        x_slice = x[n, c, h_start:h_end, w_start:w_end]&#10;                        max_idx = np.unravel_index(np.argmax(x_slice), x_slice.shape)&#10;                        self.dx[n, c, h_start + max_idx[0], w_start + max_idx[1]] += dout[n, c, i, j]&#10;&#10;        return self.dx&#10;        &#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;" />
              <option name="updatedContent" value="&quot;&quot;&quot;&#10;Otto-Friedrich University of Bamberg&#10;Chair of Explainable Machine Learning (xAI)&#10;Deep Learning Assignments&#10;&#10;@description:&#10;Implementation of the max pooling operation.&#10;&#10;@author: Sebastian Doerrich&#10;@copyright: Copyright (c) 2022, Chair of Explainable Machine Learning (xAI), Otto-Friedrich University of Bamberg&#10;@credits: [Christian Ledig, Sebastian Doerrich]&#10;@license: CC BY-SA&#10;@version: 1.0&#10;@python: Python 3&#10;@maintainer: Sebastian Doerrich&#10;@email: sebastian.doerrich@uni-bamberg.de&#10;@status: Production&#10;&quot;&quot;&quot;&#10;&#10;import numpy as np&#10;&#10;&#10;class MaxPooling:&#10;    &quot;&quot;&quot;&#10;    Max Pooling of input&#10;    &quot;&quot;&quot;&#10;    def __init__(self, kernel_size, stride):&#10;        self.kernel_size = kernel_size&#10;        self.stride = stride&#10;        self.cache = None&#10;        self.dx = None&#10;&#10;    def forward(self, x):&#10;        &quot;&quot;&quot;&#10;        Forward pass of max pooling.&#10;&#10;        :param x: input, (N, C, H, W)&#10;&#10;        :return: The output by max pooling with kernel_size and stride&#10;        &quot;&quot;&quot;&#10;&#10;        out = None&#10;        H_out, W_out = None, None&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the max pooling forward pass.                             #&#10;        #                                                                           #&#10;        # Hint:                                                                     #&#10;        #    1) You may implement the process with loops                            #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        HH, WW = self.kernel_size, self.kernel_size&#10;        stride = self.stride&#10;        H_out = (H - HH) // stride + 1&#10;        W_out = (W - WW) // stride + 1&#10;&#10;        out = np.zeros((N, C, H_out, W_out))&#10;&#10;        for n in range(N):&#10;            for c in range(C):&#10;                for i in range(H_out):&#10;                    for j in range(W_out):&#10;                        h_start, w_start = i * stride, j * stride&#10;                        h_end, w_end = h_start + HH, w_start + WW&#10;                        out[n, c, i, j] = np.max(x[n, c, h_start:h_end, w_start:w_end])&#10;&#10;        H_out, W_out = out.shape[2], out.shape[3]&#10;&#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################&#10;        self.cache = (x, H_out, W_out)&#10;        return out&#10;&#10;    def backward(self, dout):&#10;        &quot;&quot;&quot;&#10;        Backward pass of max pooling.&#10;&#10;        :param dout: Upstream derivatives&#10;        &quot;&quot;&quot;&#10;&#10;        x, H_out, W_out = self.cache&#10;&#10;        #############################################################################&#10;        # TODO:                                                                     #&#10;        #    1) Implement the max pooling backward pass.                            #&#10;        #                                                                           #&#10;        # Hint:                                                                     #&#10;        #    1) You may implement the process with loops                            #&#10;        #    2) You may find np.unravel_index useful                                #&#10;        #############################################################################&#10;&#10;        N, C, H, W = x.shape&#10;        HH, WW = self.kernel_size, self.kernel_size&#10;        stride = self.stride&#10;        H_out, W_out = dout.shape[2], dout.shape[3]&#10;&#10;        self.dx = np.zeros_like(x)&#10;&#10;        for n in range(N):&#10;            for c in range(C):&#10;                for i in range(H_out):&#10;                    for j in range(W_out):&#10;                        h_start, w_start = i * stride, j * stride&#10;                        h_end, w_end = h_start + HH, w_start + WW&#10;                        x_slice = x[n, c, h_start:h_end, w_start:w_end]&#10;                        max_idx = np.unravel_index(np.argmax(x_slice), x_slice.shape)&#10;                        self.dx[n, c, h_start + max_idx[0], w_start + max_idx[1]] += dout[n, c, i, j]&#10;&#10;        return self.dx&#10;        &#10;        #############################################################################&#10;        #                              END OF YOUR CODE                             #&#10;        #############################################################################" />
            </PendingDiffInfo>
          </value>
        </entry>
      </map>
    </option>
  </component>
</project>